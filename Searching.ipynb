{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching - A practical exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to help you understand different searching algorithms and their performance. There is a lot of code in the notebook that you do not need to worry about. The only things you should worry about are the 2 searching algorithms you will have to implement. These are:\n",
    "- Linear search\n",
    "- Binary search\n",
    "\n",
    "Do not worry if you cannot complete these in the lesson. They are purely for you to get a bit more experience with different searching algorithms. If you are really interested, these notebooks are accessible from home as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a collection of functions that will be useful for the rest of the exercise. You do not need to worry about these functions.\n",
    "\n",
    "You'll need to run this cell to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so our plots get drawn in the notebook\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from random import randint\n",
    "from time import clock\n",
    "from random import randrange\n",
    "import numpy as np\n",
    "\n",
    "# a timer - runs the provided function and reports the\n",
    "# run time in ms\n",
    "def time_f(f):\n",
    "    before = clock()\n",
    "    f()\n",
    "    after = clock()\n",
    "    return after - before\n",
    "\n",
    "def reject_outliers(data):\n",
    "    m = 1.5\n",
    "    u = np.mean(data)\n",
    "    s = np.std(data)\n",
    "    filtered = [e for e in data if (u - m * s < e < u + m * s)]\n",
    "    return filtered\n",
    "\n",
    "def time_searches(lower, upper, steps, searches):\n",
    "    # Create a list with lists of times for each searching algorithm\n",
    "    times = [[] for _ in range(len(searches) + 1)]\n",
    "    \n",
    "    # Loop to a list of size n\n",
    "    for i in range(lower, upper, steps):\n",
    "        # Apply each search \n",
    "        for search_index, search in enumerate(searches):\n",
    "            unavg_time = []\n",
    "            # Average the time over 500 searches of the list\n",
    "            for j in range(500):\n",
    "                ordered_list_temp = [num for num in range(i)]\n",
    "                runtime = time_f(lambda: search(ordered_list_temp, randint(0, i-1)))\n",
    "                unavg_time.append(runtime)\n",
    "            \n",
    "            # Calculate and save the average runtime (without outliers)\n",
    "            unavg_time = reject_outliers(unavg_time)\n",
    "            avg_runtime = sum(unavg_time) / len(unavg_time)\n",
    "            times[search_index].append(avg_runtime)\n",
    "    return times\n",
    "\n",
    "def benchmark_searches(searches, lower, upper, steps):\n",
    "    # Get list of search names\n",
    "    search_labels = [search.__name__ for search in searches]\n",
    "    # Calculate search times\n",
    "    times = time_searches(lower, upper, steps, searches)\n",
    "    \n",
    "    # Plot each searching algorithm with its name\n",
    "    for index, search_label in enumerate(search_labels):\n",
    "        plt.plot(range(lower, upper, steps), times[index], label=search_label)\n",
    "        \n",
    "    # Add axis labels and legend\n",
    "    plt.xlabel('n')\n",
    "    plt.ylabel('time (/s)')\n",
    "    plt.legend(search_labels)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def test_search(search_func):\n",
    "    # Build an ordered array of values\n",
    "    n = 100\n",
    "    x = [i for i in range(n)]\n",
    "    act_index = randint(0, n-1)\n",
    "\n",
    "    search_name = search_func.__name__\n",
    "    positive_success = search_func(x, x[act_index]) == act_index\n",
    "    negative_success = search_func(x, n+1) == None\n",
    "    \n",
    "    \n",
    "    if positive_success and negative_success:\n",
    "        print(search_name + \" works!\")\n",
    "    else:\n",
    "        print(search_name + \" has failed.\")\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Linear search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you are asked to implement `linear_search`. You have been provided with test cases to see if your implementation works. Do not change the function defintion. You should return the first index the value occurs at in the list or `None` if the value does not occur in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_search(a, val):    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this test to confirm your implementation is correct. Do not edit the test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_search(linear_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Binary Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you are asked to implement `linear_search`. You have been provided with test cases to see if your implementation works. Do not change the function defintion. You should return the first index the value occurs at in the list or `None` if the value does not occur in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(a, val):\n",
    "    return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this test to confirm your implementation is correct. Do not edit the test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_search(binary_search)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the running time performance of linear search and binary search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will compare the running times of linear search and binary search. You do not need to make any modifications to this code. Just run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_searches([linear_search, binary_search], 1, 1000, 100)\n",
    "benchmark_searches([linear_search, binary_search], 1, 10000, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that binary search runs much faster than linear search. So why don't we always use binary search? Remember, binary search only works on sorted lists!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Never write your own search!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we've just seen how you can, it's important to note that you should never write your own search! This is because the inbuilt sort often makes use of extremely advanced features that make it faster than something we could write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "def inbuilt_binary_search(a,val):\n",
    "    return bisect_left(a, val)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the benchmark below to see the performance difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_searches([binary_search, inbuilt_binary_search], 1, 10000, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
